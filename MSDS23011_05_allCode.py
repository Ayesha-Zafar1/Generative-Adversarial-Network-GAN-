# -*- coding: utf-8 -*-
"""Untitled36.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OoKx5tm3fVqiT-H6QlQKccS-Pj9qiAhN
"""

import os
from os import listdir
from os.path import isfile, join
import sys
from xml.etree import ElementTree
from xml.etree.ElementTree import Element, SubElement
from lxml import etree
import codecs
import cv2
XML_EXT = '.xml'
ENCODE_METHOD = "UTF8"
from google.colab import drive
from torch.utils.data import DataLoader
import torch
from torch.utils.data import Dataset
import torch.nn as nn
import torch.optim as optim
from torchvision import transforms

#Mounting to the google drive
drive.mount('/content/drive')

class PascalVocReader:

    def __init__(self, file_path):
        # shapes type:
        # [labbel, [(x1,y1), (x2,y2), (x3,y3), (x4,y4)], color, color, difficult]
        self.shapes = []
        self.file_path = file_path
        self.verified = False
        try:
            self.parse_xml()
        except:
            pass

    def get_shapes(self):
        return self.shapes

    def add_shape(self, label, bnd_box, difficult):
        x_min = int(float(bnd_box.find('xmin').text))
        y_min = int(float(bnd_box.find('ymin').text))
        x_max = int(float(bnd_box.find('xmax').text))
        y_max = int(float(bnd_box.find('ymax').text))
        points = [(x_min, y_min), (x_max, y_min), (x_max, y_max), (x_min, y_max)]
        self.shapes.append((label, points, None, None, difficult))

    def parse_xml(self):
        assert self.file_path.endswith(XML_EXT), "Unsupported file format"
        parser = etree.XMLParser(encoding=ENCODE_METHOD)
        xml_tree = ElementTree.parse(self.file_path, parser=parser).getroot()
        filename = xml_tree.find('filename').text
        try:
            verified = xml_tree.attrib['verified']
            if verified == 'yes':
                self.verified = True
        except KeyError:
            self.verified = False

        for object_iter in xml_tree.findall('object'):
            bnd_box = object_iter.find("bndbox")
            label = object_iter.find('name').text
            # Add chris
            difficult = False
            if object_iter.find('difficult') is not None:
                difficult = bool(int(object_iter.find('difficult').text))
            self.add_shape(label, bnd_box, difficult)
        return True
# font = cv2.FONT_HERSHEY_SIMPLEX
# color=(0,0,255)
# annotations=r"/content/drive/My Drive/400/Annotation/HCM/train"     #Annotations folder
# images=r"/content/drive/My Drive/400/HCM/train"     #Images folder
# results=r"/content/drive/My Drive/results"     #Folder where you want to store results

# files=[f for f in listdir(annotations)if isfile(join(annotations,f))]

# for f in files:
#     xml_path=join(annotations,f)
#     thickness=0
#     resolution=f.split('_')[-1].split('.')[0]
#     if resolution=="1000x":
#         thickness=33
#     elif resolution=="400x":
#         thickness=14
#     else:
#         thickness=3
# #     print(f)
#     img_name=f.split('x')[0]+'x.png'
#     img_path=join(images,img_name)
# #     print(img_path)
#     # print(xml_path)
#     img=cv2.imread(img_path)
#     t_voc_parse_reader = PascalVocReader(xml_path)
#     shapes = t_voc_parse_reader.get_shapes()
#     # print(len(shapes))
#     for s in shapes:
#         org=s[1][0]
#         org=(org[0], org[1]-8)
#         color=(0,0,255)
#         if s[0]=="ring":
#             color=(0,0,255)
#         elif s[0]=="trophozoite":
#             color=(255,0,0)
#         elif s[0]=="schizont":
#             color=(0,255,0)
#         else:
#             color=(0,0,0)
# # you can add the label on each boundary box plotted.
# #         img = cv2.putText(img, s[0], org, font,
# #                     1, color, 2, cv2.LINE_AA)
#         cv2.rectangle(img,s[1][0],s[1][2],color,thickness)

#     cv2.imwrite(join(results,img_name),img)

# font = cv2.FONT_HERSHEY_SIMPLEX
# color=(0,0,255)
# annotations=r"/content/drive/My Drive/400/Annotation/HCM/val"     #Annotations folder
# images=r"/content/drive/My Drive/400/HCM/val"     #Images folder
# results_val=r"/content/drive/My Drive/results_val"     #Folder where you want to store results

# files=[f for f in listdir(annotations)if isfile(join(annotations,f))]

# for f in files:
#     xml_path=join(annotations,f)
#     thickness=0
#     resolution=f.split('_')[-1].split('.')[0]
#     if resolution=="1000x":
#         thickness=33
#     elif resolution=="400x":
#         thickness=14
#     else:
#         thickness=3
# #     print(f)
#     img_name=f.split('x')[0]+'x.png'
#     img_path=join(images,img_name)
# #     print(img_path)
#     # print(xml_path)
#     img=cv2.imread(img_path)
#     t_voc_parse_reader = PascalVocReader(xml_path)
#     shapes = t_voc_parse_reader.get_shapes()
#     # print(len(shapes))
#     for s in shapes:
#         org=s[1][0]
#         org=(org[0], org[1]-8)
#         color=(0,0,255)
#         if s[0]=="ring":
#             color=(0,0,255)
#         elif s[0]=="trophozoite":
#             color=(255,0,0)
#         elif s[0]=="schizont":
#             color=(0,255,0)
#         else:
#             color=(0,0,0)
# # you can add the label on each boundary box plotted.
# #         img = cv2.putText(img, s[0], org, font,
# #                     1, color, 2, cv2.LINE_AA)
#         cv2.rectangle(img,s[1][0],s[1][2],color,thickness)

#     cv2.imwrite(join(results_val,img_name),img)

# font = cv2.FONT_HERSHEY_SIMPLEX
# color=(0,0,255)
# annotations=r"/content/drive/My Drive/400/Annotation/HCM/test"     #Annotations folder
# images=r"/content/drive/My Drive/400/HCM/test"     #Images folder
# results_test=r"/content/drive/My Drive/results_test"     #Folder where you want to store results

# files=[f for f in listdir(annotations)if isfile(join(annotations,f))]

# for f in files:
#     xml_path=join(annotations,f)
#     thickness=0
#     resolution=f.split('_')[-1].split('.')[0]
#     if resolution=="1000x":
#         thickness=33
#     elif resolution=="400x":
#         thickness=14
#     else:
#         thickness=3
# #     print(f)
#     img_name=f.split('x')[0]+'x.png'
#     img_path=join(images,img_name)
# #     print(img_path)
#     # print(xml_path)
#     img=cv2.imread(img_path)
#     t_voc_parse_reader = PascalVocReader(xml_path)
#     shapes = t_voc_parse_reader.get_shapes()
#     # print(len(shapes))
#     for s in shapes:
#         org=s[1][0]
#         org=(org[0], org[1]-8)
#         color=(0,0,255)
#         if s[0]=="ring":
#             color=(0,0,255)
#         elif s[0]=="trophozoite":
#             color=(255,0,0)
#         elif s[0]=="schizont":
#             color=(0,255,0)
#         else:
#             color=(0,0,0)
# # you can add the label on each boundary box plotted.
# #         img = cv2.putText(img, s[0], org, font,
# #                     1, color, 2, cv2.LINE_AA)
#         cv2.rectangle(img,s[1][0],s[1][2],color,thickness)

#     cv2.imwrite(join(results_test,img_name),img)

"""## Implementation"""

class MalariaDataset(Dataset):
    def __init__(self, root_dir, mode='classification', transform=None):
        self.root_dir = root_dir
        self.mode = mode
        self.transform = transform
        self.image_folder = os.path.join(root_dir, 'HCM', 'train')
        self.annotation_folder = os.path.join(root_dir, 'Annotation', 'HCM', 'train')
        self.image_files = [f for f in os.listdir(self.image_folder) if os.path.isfile(os.path.join(self.image_folder, f))]
        self.labels = self._load_labels()

    def _load_labels(self):
        labels = {}
        for filename in os.listdir(self.annotation_folder):
            if filename.endswith('.xml'):
                label = self._parse_annotation_file(os.path.join(self.annotation_folder, filename))
                image_name = filename.split('.')[0] + 'x.png'
                labels[image_name] = label
        return labels

    # def _parse_annotation_file(self, filepath):
    #     # Implement parsing of annotation file to extract label
    #     # You can use the provided PascalVocReader class or any other method to parse XML
    #     pass

    def _parse_annotation_file(self, filepath):

      voc_reader = PascalVocReader(filepath)
      shapes = voc_reader.get_shapes()
      label = None
      for s in shapes:
          label = s[0]  # Taking the first shape in the annotation file is the label

          break  
      return label


    def __len__(self):
        return len(self.image_files)

    def __getitem__(self, idx):
        img_name = self.image_files[idx]
        img_path = os.path.join(self.image_folder, img_name)
        image = cv2.imread(img_path)
        if image is None:
            raise FileNotFoundError(f"Image not found: {img_path}")

        if self.transform:
            image = self.transform(image)

        if self.mode == 'classification':
            label = self.labels.get(img_name, -1)  # Get label from dictionary, default -1 if not found
            return image, label
        elif self.mode == 'GAN':
            return image
        else:
            raise ValueError("Invalid mode. Supported modes are 'classification' and 'GAN'.")

class Generator(nn.Module):
    def __init__(self, latent_dim=100, channels=3, image_size=128):
        super(Generator, self).__init__()
        self.latent_dim = latent_dim
        self.channels = channels
        self.image_size = image_size

        self.main = nn.Sequential(
            nn.ConvTranspose2d(self.latent_dim, 512, 4, 1, 0, bias=False),
            nn.BatchNorm2d(512),
            nn.ReLU(True),

            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias=False),
            nn.BatchNorm2d(256),
            nn.ReLU(True),

            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False),
            nn.BatchNorm2d(128),
            nn.ReLU(True),

            nn.ConvTranspose2d(128, self.channels, 4, 2, 1, bias=False),
            nn.Tanh()
        )

    def forward(self, z):
        z = z.view(z.size(0), self.latent_dim, 1, 1)
        img = self.main(z)
        return img


class Discriminator(nn.Module):
    def __init__(self, channels=3, image_size=128):
        super(Discriminator, self).__init__()
        self.channels = channels
        self.image_size = image_size

        self.main = nn.Sequential(
            nn.Conv2d(self.channels, 128, 4, 2, 1, bias=False),
            nn.LeakyReLU(0.2, inplace=True),

            nn.Conv2d(128, 256, 4, 2, 1, bias=False),
            nn.BatchNorm2d(256),
            nn.LeakyReLU(0.2, inplace=True),

            nn.Conv2d(256, 512, 4, 2, 1, bias=False),
            nn.BatchNorm2d(512),
            nn.LeakyReLU(0.2, inplace=True),

            nn.Conv2d(512, 1, 4, 1, 0, bias=False),
            nn.Sigmoid()
        )

    def forward(self, img):
        validity = self.main(img)
        return validity.view(-1, 1).squeeze(1)

import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt
import os

def train_GAN_(dataloader, generator, discriminator, optimizer_G, optimizer_D, device, num_epochs, latent_dim, save_dir):
    criterion = nn.BCELoss()
    gen_losses = []
    disc_losses = []

    os.makedirs(save_dir, exist_ok=True)

    for epoch in range(num_epochs):
        for i, real_images in enumerate(dataloader):
            real_images = real_images.to(device)
            batch_size = real_images.size(0)

            # Train Discriminator
            optimizer_D.zero_grad()

            # Train with real images
            output_real = discriminator(real_images)
            labels_real = torch.ones_like(output_real)
            loss_D_real = criterion(output_real, labels_real)

            # Train with fake images
            noise = torch.randn(batch_size, latent_dim, device=device)
            fake_images = generator(noise)
            output_fake = discriminator(fake_images.detach())
            labels_fake = torch.zeros_like(output_fake)
            loss_D_fake = criterion(output_fake, labels_fake)

            # Update discriminator parameters
            loss_D = loss_D_real + loss_D_fake
            loss_D.backward()
            optimizer_D.step()

            # Train Generator
            optimizer_G.zero_grad()
            output = discriminator(fake_images)
            labels_real = torch.ones_like(output)  # Generator wants to fool discriminator
            loss_G = criterion(output, labels_real)

            # Update generator parameters
            loss_G.backward()
            optimizer_G.step()

            # Store losses
            gen_losses.append(loss_G.item())
            disc_losses.append(loss_D.item())

            if i % 100 == 0:
                print(
                    f"Epoch [{epoch+1}/{num_epochs}]"
                    f"Loss D Real: {loss_D_real.item():.4f}, Loss D Fake: {loss_D_fake.item():.4f}, "
                    f"Loss G: {loss_G.item():.4f}"
                )

        # Save generated images every epoch
        save_generated_images(generator, device, latent_dim, save_dir, epoch)

    # Plot and save losses
    plot_losses(gen_losses, disc_losses, save_dir)


    return generator, discriminator, gen_losses, disc_losses

def save_generated_images(generator, device, latent_dim, save_dir, epoch, num_samples=10):
    noise = torch.randn(num_samples, latent_dim, device=device)
    with torch.no_grad():
        fake_images = generator(noise).cpu()

    # Rescale images to range [0, 1]
    fake_images = (fake_images + 1) / 2

    # Save images
    os.makedirs(os.path.join(save_dir, "generated_images"), exist_ok=True)
    for i, img in enumerate(fake_images):
        img_path = os.path.join(save_dir, "generated_images", f"epoch_{epoch+1}_image_{i+1}.png")
        plt.imsave(img_path, img.permute(1, 2, 0).numpy())

def plot_losses(gen_losses, disc_losses, save_dir):
    plt.figure(figsize=(10, 5))
    plt.plot(gen_losses, label='Generator Loss', color='blue')
    plt.plot(disc_losses, label='Discriminator Loss', color='red')
    plt.xlabel('Iterations')
    plt.ylabel('Loss')
    plt.title('Generator and Discriminator Losses')
    plt.legend()
    plt.grid(True)
    plt.savefig(os.path.join(save_dir, "losses_plot.png"))
    plt.show()


def plot_weightss(generator, discriminator, save_dir):
    # Plot and save weights of discriminator and generator
    plt.figure(figsize=(10, 5))
    plt.subplot(1, 2, 1)
    plt.title("Generator Weights")
    for param_tensor in generator.parameters():
        plt.hist(param_tensor.cpu().detach().numpy().flatten(), bins=50, color='blue')
    plt.subplot(1, 2, 2)
    plt.title("Discriminator Weights")
    for param_tensor in discriminator.parameters():
        plt.hist(param_tensor.cpu().detach().numpy().flatten(), bins=50, color='red')
    plt.savefig(os.path.join(save_dir, "weightss_plot.png"))
    plt.show()


# Define hyperparameters
batch_size = 64
latent_dim = 100
num_epochs = 10
image_size = 128
learning_rate_G = 0.0001
learning_rate_D = 0.0001


# Define transformations
transform = transforms.Compose([
    transforms.ToPILImage(),
    transforms.Resize((image_size, image_size)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize to [-1, 1] range
])

# Set device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Initialize dataset and dataloader
dataset = MalariaDataset(root_dir="/content/drive/My Drive/400", mode='GAN', transform=transform)
dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=2)

# Initialize Generator and Discriminator
generator = Generator(latent_dim=latent_dim, channels=3, image_size=image_size).to(device)
discriminator = Discriminator(channels=3, image_size=image_size).to(device)

# Initialize optimizers
optimizer_G = optim.Adam(generator.parameters(), lr=learning_rate_G, betas=(0.5, 0.999))
optimizer_D = optim.Adam(discriminator.parameters(), lr=learning_rate_D, betas=(0.5, 0.999))




# Train GAN
save_dir = "/content/drive/My Drive/save_dir"
generator, discriminator, gen_losses, disc_losses = train_GAN_(dataloader, generator, discriminator, optimizer_G, optimizer_D, device, num_epochs, latent_dim, save_dir)

# Plot and save weights of discriminator and generator
plot_weightss(generator, discriminator, save_dir)



import torch
import matplotlib.pyplot as plt

def test_GAN(generator, latent_dim, num_samples=10, device='cpu'):
    # Generate fake images
    with torch.no_grad():
        noise = torch.randn(num_samples, latent_dim, device=device)
        fake_images = generator(noise).cpu()

    # Plot the generated images
    plt.figure(figsize=(10, 10))
    for i in range(num_samples):
        plt.subplot(1, num_samples, i + 1)
        plt.imshow(fake_images[i].permute(1, 2, 0))
        plt.axis('off')
    plt.tight_layout()
    plt.show()


latent_dim = 100  
test_GAN(generator, latent_dim)

#-----------------TASK02-----------------------#
# -*- coding: utf-8 -*-
"""Untitled36.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ym6ebA3W9p1POMJp2vB83BOnQzvGACQb
"""

import os
from os import listdir
from os.path import isfile, join
import sys
from xml.etree import ElementTree
from xml.etree.ElementTree import Element, SubElement
from lxml import etree
import codecs
import cv2
XML_EXT = '.xml'
ENCODE_METHOD = "UTF8"
from google.colab import drive
from torch.utils.data import DataLoader
import torch
from torch.utils.data import Dataset
import torch.nn as nn
import torch.optim as optim
from torchvision import transforms
import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import accuracy_score

#Mounting to the google drive
drive.mount('/content/drive')

class PascalVocReader:

    def __init__(self, file_path):
        # shapes type:
        # [labbel, [(x1,y1), (x2,y2), (x3,y3), (x4,y4)], color, color, difficult]
        self.shapes = []
        self.file_path = file_path
        self.verified = False
        try:
            self.parse_xml()
        except:
            pass

    def get_shapes(self):
        return self.shapes

    def add_shape(self, label, bnd_box, difficult):
        x_min = int(float(bnd_box.find('xmin').text))
        y_min = int(float(bnd_box.find('ymin').text))
        x_max = int(float(bnd_box.find('xmax').text))
        y_max = int(float(bnd_box.find('ymax').text))
        points = [(x_min, y_min), (x_max, y_min), (x_max, y_max), (x_min, y_max)]
        self.shapes.append((label, points, None, None, difficult))

    def parse_xml(self):
        assert self.file_path.endswith(XML_EXT), "Unsupported file format"
        parser = etree.XMLParser(encoding=ENCODE_METHOD)
        xml_tree = ElementTree.parse(self.file_path, parser=parser).getroot()
        filename = xml_tree.find('filename').text
        try:
            verified = xml_tree.attrib['verified']
            if verified == 'yes':
                self.verified = True
        except KeyError:
            self.verified = False

        for object_iter in xml_tree.findall('object'):
            bnd_box = object_iter.find("bndbox")
            label = object_iter.find('name').text
            # Add chris
            difficult = False
            if object_iter.find('difficult') is not None:
                difficult = bool(int(object_iter.find('difficult').text))
            self.add_shape(label, bnd_box, difficult)
        return True
# font = cv2.FONT_HERSHEY_SIMPLEX
# color=(0,0,255)
# annotations=r"/content/drive/My Drive/400/Annotation/HCM/train"     #Annotations folder
# images=r"/content/drive/My Drive/400/HCM/train"     #Images folder
# results=r"/content/drive/My Drive/results"     #Folder where you want to store results

# files=[f for f in listdir(annotations)if isfile(join(annotations,f))]

# for f in files:
#     xml_path=join(annotations,f)
#     thickness=0
#     resolution=f.split('_')[-1].split('.')[0]
#     if resolution=="1000x":
#         thickness=33
#     elif resolution=="400x":
#         thickness=14
#     else:
#         thickness=3
# #     print(f)
#     img_name=f.split('x')[0]+'x.png'
#     img_path=join(images,img_name)
# #     print(img_path)
#     # print(xml_path)
#     img=cv2.imread(img_path)
#     t_voc_parse_reader = PascalVocReader(xml_path)
#     shapes = t_voc_parse_reader.get_shapes()
#     # print(len(shapes))
#     for s in shapes:
#         org=s[1][0]
#         org=(org[0], org[1]-8)
#         color=(0,0,255)
#         if s[0]=="ring":
#             color=(0,0,255)
#         elif s[0]=="trophozoite":
#             color=(255,0,0)
#         elif s[0]=="schizont":
#             color=(0,255,0)
#         else:
#             color=(0,0,0)
# # you can add the label on each boundary box plotted.
# #         img = cv2.putText(img, s[0], org, font,
# #                     1, color, 2, cv2.LINE_AA)
#         cv2.rectangle(img,s[1][0],s[1][2],color,thickness)

#     cv2.imwrite(join(results,img_name),img)

# Define DataLoader Class
class MalariaDataset(Dataset):
    class_mapping = {"ring": 0, "trophozoite": 1, "schizont": 2, "gametocyte": 3}  # Map class labels to integers

    def __init__(self, root_dir, mode='classification', transform=None):
        self.root_dir = root_dir
        self.mode = mode
        self.transform = transform
        self.image_folder = os.path.join(root_dir, 'HCM', 'train')
        self.annotation_folder = os.path.join(root_dir, 'Annotation', 'HCM', 'train')
        self.image_files = [f for f in os.listdir(self.image_folder) if os.path.isfile(os.path.join(self.image_folder, f))]
        self.labels = self._load_labels()

    def _load_labels(self):
        labels = {}
        for filename in os.listdir(self.annotation_folder):
            if filename.endswith('.xml'):
                label = self._parse_annotation_file(os.path.join(self.annotation_folder, filename))
                image_name = filename.split('.')[0] + 'x.png'
                labels[image_name] = label
        return labels

    def _parse_annotation_file(self, filepath):
        voc_reader = PascalVocReader(filepath)
        shapes = voc_reader.get_shapes()
        num_classes = len(self.class_mapping)
        label = np.zeros(num_classes)  # Initialize label as all zeros
        for s in shapes:
            class_index = self.class_mapping.get(s[0], -1)  # Get the index of the class label
            if class_index != -1:
                label[class_index] = 1  # Set the corresponding index to 1 if class is present
        return label

    def __len__(self):
        return len(self.image_files)

    def __getitem__(self, idx):
        img_name = self.image_files[idx]
        img_path = os.path.join(self.image_folder, img_name)
        image = cv2.imread(img_path)
        if image is None:
            raise FileNotFoundError(f"Image not found: {img_path}")

        if self.transform:
            image = self.transform(image)

        if self.mode == 'classification':
            label = self.labels.get(img_name, np.zeros(len(self.class_mapping)))  # Get label from dictionary, default all zeros if not found
            return image, label
        elif self.mode == 'GAN':
            return image
        else:
            raise ValueError("Invalid mode. Supported modes are 'classification' and 'GAN'.")

# Define MultiLabelClassifier Class
class MultiLabelClassifier(nn.Module):
    def __init__(self, num_classes, channels=3, image_size=128):
        super(MultiLabelClassifier, self).__init__()
        self.channels = channels
        self.image_size = image_size

        self.features = nn.Sequential(
            nn.Conv2d(self.channels, 128, 4, 2, 1, bias=False),
            nn.LeakyReLU(0.2, inplace=True),

            nn.Conv2d(128, 256, 4, 2, 1, bias=False),
            nn.BatchNorm2d(256),
            nn.LeakyReLU(0.2, inplace=True),

            nn.Conv2d(256, 512, 4, 2, 1, bias=False),
            nn.BatchNorm2d(512),
            nn.LeakyReLU(0.2, inplace=True),
        )

        self.classifier = nn.Sequential(
            nn.Linear(512 * (self.image_size // 8) * (self.image_size // 8), 512),
            nn.ReLU(inplace=True),
            nn.Linear(512, num_classes),
            nn.Sigmoid()  # Sigmoid activation for multi-label classification
        )

        # Placeholder to store the last convolutional layer feature maps
        self.last_conv_features = None

    def forward(self, img):
        features = self.features(img)
        self.last_conv_features = features.clone().detach()  # Store feature maps for GradCAM
        output = self.classifier(features.view(features.size(0), -1))
        return output

    def get_activations_gradient(self):
        return self.last_conv_features.grad

    def get_activations(self, img):
        return self.features(img)

    def guided_backprop(self, img, target_class):
        self.zero_grad()
        output = self.forward(img)
        pred_class = (output > 0.5).int()
        if torch.equal(pred_class, target_class):
            output.backward()
            return img.grad.data
        else:
            return None

# Define test_DataLoader Class
class MalariaDataset_test_data(Dataset):
    class_mapping = {"ring": 0, "trophozoite": 1, "schizont": 2, "gametocyte": 3}  # Map class labels to integers

    def __init__(self, root_dir, mode='classification', transform=None):
        self.root_dir = root_dir
        self.mode = mode
        self.transform = transform
        self.image_folder = os.path.join(root_dir, 'HCM', 'test')
        self.annotation_folder = os.path.join(root_dir, 'Annotation', 'HCM', 'test')
        self.image_files = [f for f in os.listdir(self.image_folder) if os.path.isfile(os.path.join(self.image_folder, f))]
        self.labels = self._load_labels()

    def _load_labels(self):
        labels = {}
        for filename in os.listdir(self.annotation_folder):
            if filename.endswith('.xml'):
                label = self._parse_annotation_file(os.path.join(self.annotation_folder, filename))
                image_name = filename.split('.')[0] + 'x.png'
                labels[image_name] = label
        return labels

    def _parse_annotation_file(self, filepath):
        voc_reader = PascalVocReader(filepath)
        shapes = voc_reader.get_shapes()
        num_classes = len(self.class_mapping)
        label = np.zeros(num_classes)  # Initialize label as all zeros
        for s in shapes:
            class_index = self.class_mapping.get(s[0], -1)  # Get the index of the class label
            if class_index != -1:
                label[class_index] = 1  # Set the corresponding index to 1 if class is present
        return label

    def __len__(self):
        return len(self.image_files)

    def __getitem__(self, idx):
        img_name = self.image_files[idx]
        img_path = os.path.join(self.image_folder, img_name)
        image = cv2.imread(img_path)
        if image is None:
            raise FileNotFoundError(f"Image not found: {img_path}")

        if self.transform:
            image = self.transform(image)

        if self.mode == 'classification':
            label = self.labels.get(img_name, np.zeros(len(self.class_mapping)))  # Get label from dictionary, default all zeros if not found
            return image, label
        elif self.mode == 'GAN':
            return image
        else:
            raise ValueError("Invalid mode. Supported modes are 'classification' and 'GAN'.")

# Define train function for MultiLabelClassifier
def train_classifier(dataloader, classifier, criterion, optimizer, device, num_epochs, save_dir):
    losses = []
    accuracies = []

    os.makedirs(save_dir, exist_ok=True)

    for epoch in range(num_epochs):
        epoch_loss = 0.0
        predicted_labels = []
        true_labels = []

        for i, (images, labels) in enumerate(dataloader):
            images, labels = images.to(device), labels.to(device)

            optimizer.zero_grad()

            outputs = classifier(images)
            #loss = criterion(outputs, labels)
            loss = criterion(outputs, labels.float())  # Convert labels to Float
            loss.backward()
            optimizer.step()

            epoch_loss += loss.item()

            predicted_labels.extend(outputs.cpu().detach().numpy())
            true_labels.extend(labels.cpu().detach().numpy())

        epoch_loss /= len(dataloader)
        losses.append(epoch_loss)

        predicted_labels = np.array(predicted_labels)
        true_labels = np.array(true_labels)

        accuracy = accuracy_score(true_labels, (predicted_labels > 0.5).astype(int))
        accuracies.append(accuracy)

        print(f"Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {accuracy:.4f}")

    # Save model
    torch.save(classifier.state_dict(), os.path.join(save_dir, 'classifier.pth'))

    return classifier, losses, accuracies

def plot_losses(cls_losses, cls_accuracies, save_dir):
    plt.figure(figsize=(10, 5))
    plt.plot(cls_losses, label='Classifier Loss', color='blue')
    plt.xlabel('Iterations')
    plt.ylabel('Loss')
    plt.title('Multi-label Classifier Loss')
    plt.legend()
    plt.grid(True)
    plt.savefig(os.path.join(save_dir, "cls_losses_plot.png"))
    plt.show()

def plot_weightss(classifier, save_dir):
    # Plot and save weights of classifier
    plt.figure(figsize=(10, 5))
    plt.title("Classifier Weights")
    for param_tensor in classifier.parameters():
        plt.hist(param_tensor.cpu().detach().numpy().flatten(), bins=50, color='blue')
    plt.savefig(os.path.join(save_dir, "cls_weightss_plot.png"))
    plt.show()

# Set random seed for reproducibility
torch.manual_seed(0)

# Define hyperparameters
batch_size = 8
num_classes = 4  # Number of classes (Ring, Trophozoite, Schizont, Gametocyte)
num_epochs = 10
learning_rate = 0.01
image_size = 128

# Define transformations
transform = transforms.Compose([
    transforms.ToPILImage(),
    transforms.Resize((image_size, image_size)),
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])

# Set device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Initialize dataset and dataloader
dataset = MalariaDataset(root_dir="/content/drive/My Drive/400", mode="classification", transform=transform)
dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

# Initialize classifier
classifier = MultiLabelClassifier(num_classes=num_classes, channels=3, image_size=image_size).to(device)

# Define loss function and optimizer
criterion = nn.BCELoss()
optimizer = optim.Adam(classifier.parameters(), lr=learning_rate)

# Train classifier
#train_classifier(dataloader, classifier, criterion, optimizer, device, num_epochs, save_dir="/content/drive/My Drive/save_dir_2")

#Train classifier
classifier, losses, accuracies = train_classifier(dataloader, classifier, criterion, optimizer, device, num_epochs, save_dir="/content/drive/My Drive/save_dir_2")

plot_losses(losses, accuracies, save_dir="/content/drive/My Drive/save_dir_2")

plot_weightss(classifier, save_dir="/content/drive/My Drive/save_dir_2")

def test_classifier(dataloader, classifier, device):
    classifier.eval()
    predicted_labels = []
    true_labels = []

    with torch.no_grad():
        for images, labels in dataloader:
            images, labels = images.to(device), labels.to(device)
            outputs = classifier(images)
            predicted_labels.extend(outputs.cpu().detach().numpy())
            true_labels.extend(labels.cpu().detach().numpy())

    predicted_labels = np.array(predicted_labels)
    true_labels = np.array(true_labels)

    accuracy = accuracy_score(true_labels, (predicted_labels > 0.5).astype(int))
    print(f"Test Accuracy: {accuracy:.4f}")


# # Initialize test dataset and dataloader
test_dataset = MalariaDataset_test_data(root_dir="/content/drive/My Drive/400", mode="classification", transform=transform)
test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

# Test classifier
test_classifier(dataloader, classifier, device)

import cv2
import numpy as np
import matplotlib.pyplot as plt

def overlay_heatmap(image, heatmap, alpha=0.5, colormap=cv2.COLORMAP_JET):
    # Resize the heatmap to match the dimensions of the original image
    heatmap_resized = cv2.resize(heatmap, (image.shape[1], image.shape[0]))
    # Normalize the heatmap values to range [0, 1]
    heatmap_normalized = cv2.normalize(heatmap_resized, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8U)
    # Apply the colormap to the heatmap
    heatmap_colored = cv2.applyColorMap(heatmap_normalized, colormap)
    # Overlay the heatmap onto the original image using alpha blending
    overlaid_image = cv2.addWeighted(image, 1-alpha, heatmap_colored, alpha, 0)
    return overlaid_image

# Example usage
# Load the original image and heatmap
original_image = cv2.imread('/content/drive/My Drive/400/HCM/train/Malaria_CM24_28Aug2021162625_0001_144.4_13.1_400x.png')
heatmap = cv2.imread('/content/drive/My Drive/400/HCM/train/Malaria_CM24_28Aug2021162625_0001_144.4_13.1_400x.png', cv2.IMREAD_GRAYSCALE)  # Assuming heatmap is grayscale

# Overlay heatmap onto the original image
result = overlay_heatmap(original_image, heatmap)

# Display the result
plt.imshow(cv2.cvtColor(result, cv2.COLOR_BGR2RGB))
plt.axis('off')
plt.show()